---
title: "Mapping the Unseen: Small Area Estimation for Urban Analysis"
author:
  - name: Clara Peiret-García
    email: c.peiret-garcia@ucl.ac.uk
    affiliations:
      - name: Centre for Advanced Spatial Analysis, UCL
  - name: Anna Freni Sterrantino
    affiliations:
      - name: Alan Turing Institute | Imperial College London
  - name: Esra Suel
    affiliations:
      - name: Centre for Advanced Spatial Analysis, UCL
      
  - name: Adam Dennett
    affiliations:
      - name: Centre for Advanced Spatial Analysis, UCL
  - name: Gerard Casey
    affiliations:
      - name: Arup | Centre for Advanced Spatial Analysis, UCL
      
format: html
editor: visual
---

Please, make sure you have `R version 4.5.1 (2025-06-13)` installed in your laptop. You can download it from here: <https://cran.r-project.org/>

```{r, message=FALSE}
# Install required packages if not already installed
required_packages <- c(
  "sae", "emdi", "saeTrafo", "hbsae", "SUMMER", "survey",
  "dplyr", "tidyr", "purrr", "sf", 
  "ggplot2", "hrbrthemes", "GGally", "patchwork"
)

to_install <- setdiff(required_packages, rownames(installed.packages()))
if (length(to_install) > 0) {
  install.packages(to_install)
}

# Install INLA for Bayesian SAE estimators
if (!isTRUE(requireNamespace("INLA", quietly = TRUE))) {
  install.packages("INLA", repos=c(getOption("repos"), 
                  INLA="https://inla.r-inla-download.org/R/stable"), dep=TRUE)
}

library(sae)          # Small area estimation models
library(emdi)         # Example datasets
library(saeTrafo)     # For domain sizes
library(SUMMER)       # Bayesian SAE
library(survey)       # Survey designs
library(dplyr)        # For data wrangling
library(tidyr)        # For data wrangling
library(purrr)        # For data wrangling
library(sf)           # For mapping
library(ggplot2)      # For visualisations
library(hrbrthemes)   # For visualisations
library(GGally)       # For visualisations
library(patchwork)    # For visualisations
```

# Introduction

In this practical we will put into practice the concepts we learnt on the theoretical session of the workshop. Using survey data, we will calculate direct and model-based income estimators. We will explore the different alternatives available when implementing these methods, which will help us choose the most adequate option given data availability. To better understand the implications of using different models, we will compare the results of the estimates generated through different methods. You can access the `.qmd` document for this practical from [this link](https://github.com/cpeiretgarcia/SAE_workshop_CUPUM/blob/main/practical.qmd).

# Data

For this workshop we will be using the European Union Statistics on Income and Living Conditions (EU-SILC). Specifically, we will be using the Austrian EU-SILC data sets available through the `emdi` package. EU-SILC provides detailed information on attributes related to income, material deprivation, labour, housing, childcare, health, access to and use of services, and education.

From the package `emdi` we can load a set of data related to the EU-SILC survey. `eusilcA_smp` is the random independent sample, where each row represents one individual, and each column represents a unit-level attribute. In total, the sample comprises 1,945 individuals. `eusilcA_popAgg` comprises the area-level covariates for all domains. `eusilcA_pop` is the total population -- it comprises 25,000 observations which we will assume add up to the total population of Austria for this example. Finally, `eusilcA_prox` is the adjacency matrix for every district in Austria.

```{r}
# Load data
data("eusilcA_smp")     # Random independent Sample
data("eusilcA_popAgg")  # Aggregated covariates at district level
data("eusilcA_pop")     # Population level data
data("eusilcA_prox")    #Adjacency matrix
data("eusilcA_smpAgg")  #AD ADD THIS IN

str(eusilcA_smp)
str(eusilcA_popAgg)
str(eusilcA_pop)
str(eusilcA_prox)
str(eusilcA_smpAgg)

set.seed(123)  # Optional: ensures reproducibility

#########Create some Alternative Samples##########

#create a second simple random sample from the population
eusilcA_smp2 <- eusilcA_pop %>%
  sample_n(size = 1945)

#create a third sample - an alternative stratified sample

# Step 1: Count population per district
pop_counts <- eusilcA_pop %>%
  count(district, name = "pop_n")

# Step 2: Define total sample size
total_sample_size <- 1945  # adjust as needed - match it to the sample size of the first sample

# Step 3: Allocate sample size proportionally
pop_counts <- pop_counts %>%
  mutate(sample_n = round(pop_n / sum(pop_n) * total_sample_size))

# Step 4: Stratified sampling
set.seed(123)  # for reproducibility
stratified_samples <- pop_counts %>%
  rowwise() %>%
  mutate(sample = list(
    eusilcA_pop %>%
      filter(district == district) %>%
      slice_sample(n = sample_n) # this is a cool function which allows you to subset rows according to the position defined in sample_n
  ))

# Step 5: Unnest with name repair - stratified samples just selects samples from each district, but does not unnest them
eusilcA_smp3 <- stratified_samples %>%
  unnest(sample, names_repair = "unique")

# Step 6: Clean up column names
eusilcA_smp3 <- eusilcA_smp3 %>%
  rename(district = district...1) %>%
  select(-matches("district\\.\\.\\.\\d+$"))  # drops district...20 etc.

# Step 7: Add sampling weights
eusilcA_smp3 <- eusilcA_smp3 %>%
  mutate(weight = pop_n / sample_n) 


# Recode the domain variable as character
#eusilcA_smp$district <- droplevels(eusilcA_smp$district)
#eusilcA_smp$district <- as.character(eusilcA_smp$district)
```



Let us start by having a look at the first set of sample data `eusilcA_smp`. Each row in the sample represents one case - an individual, or in our case, a household reference person. And for each of them we have information on a wide range of economic and demographic attributes. In this practical, our **target variable** will be the the equivalised household income (`eqIncome`), which represents the household income adjusted by household composition characteristics.

```{r}
head(eusilcA_smp)
```

We can also have a look at the spatial distribution of the observations in the samples.

```{r}
library(dplyr)
library(ggplot2)
library(viridis)
library(leaflet)
library(sf)

# Load geospatial data and set CRS
load_shapeaustria()
shape_austria_dis <- st_set_crs(shape_austria_dis, 4326)

# Function to summarize sample by district and join to spatial data
summarize_sample <- function(sample_df, shape_df) {
  sample_df %>%
    group_by(district) %>%
    summarise(n = n(), .groups = "drop") %>%
    left_join(shape_df, by = c("district" = "PB")) %>%
    st_as_sf()
}

# Prepare summaries for each sample
smp1_map <- shape_austria_dis %>%
  left_join(eusilcA_smp %>% count(district, name = "n"), by = c("PB" = "district"))

smp2_map <- shape_austria_dis %>%
  left_join(eusilcA_smp2 %>% count(district, name = "n"), by = c("PB" = "district"))

smp3_map <- shape_austria_dis %>%
  left_join(eusilcA_smp3 %>% count(district, name = "n"), by = c("PB" = "district"))

# City labels
city_coords <- data.frame(
  name = c("Wien", "Graz", "Murau"),
  lon = c(16.3738, 15.4395, 14.1700),
  lat = c(48.2082, 47.0707, 47.1147)
)
city_points <- st_as_sf(city_coords, coords = c("lon", "lat"), crs = 4326)

# Color palette
pal <- colorNumeric("viridis", domain = NULL)

# Build leaflet map with layers
leaflet() %>%
  addTiles() %>%
  
  # Sample 1 layer
  addPolygons(
    data = smp1_map,
    fillColor = ~pal(n),
    fillOpacity = 0.7,
    color = "white",
    weight = 1,
    label = ~paste0(PB, ": ", n, " obs"),
    group = "Sample 1"
  ) %>%
  
  # Sample 2 layer
  addPolygons(
    data = smp2_map,
    fillColor = ~pal(n),
    fillOpacity = 0.7,
    color = "white",
    weight = 1,
    label = ~paste0(PB, ": ", n, " obs"),
    group = "Sample 2"
  ) %>%
  
  # Sample 3 layer
  addPolygons(
    data = smp3_map,
    fillColor = ~pal(n),
    fillOpacity = 0.7,
    color = "white",
    weight = 1,
    label = ~paste0(PB, ": ", n, " obs"),
    group = "Sample 3"
  ) %>%
  
  # City labels
  addLabelOnlyMarkers(
    data = city_points,
    label = ~name,
    labelOptions = labelOptions(
      noHide = TRUE,
      direction = "top",
      textOnly = TRUE,
      style = list(
        "color" = "white",
        "font-weight" = "bold",
        "text-shadow" = "1px 1px 2px black"
      )
    )
  ) %>%
  
  # Legend
  addLegend(
    pal = pal,
    values = c(smp1_map$n, smp2_map$n, smp3_map$n),
    title = "Sample Size",
    position = "bottomright"
  ) %>%
  
  # Layer control
  addLayersControl(
    overlayGroups = c("Sample 1", "Sample 2", "Sample 3"),
    options = layersControlOptions(collapsed = FALSE)
  )
```

```{r}
library(dplyr)
library(ggplot2)

# Tag each sample
df1 <- eusilcA_smp %>% mutate(source = "eusilcA_smp")
df2 <- eusilcA_smp2 %>% mutate(source = "eusilcA_smp2")
df3 <- eusilcA_smp3 %>% mutate(source = "eusilcA_smp3")

# Compute district order based on eusilcA_smp
district_order <- df1 %>%
  count(district) %>%
  arrange(n) %>%
  pull(district)

# Combine and reorder factor levels
combined_df <- bind_rows(df1, df2, df3) %>%
  mutate(district = factor(district, levels = district_order))

# Plot
ggplot(combined_df, aes(x = district, fill = source)) +
  geom_bar(position = "dodge") +
  coord_flip() +
  labs(
    title = "District Counts in Sampled Dataframes",
    subtitle = "Comparison of Sampling Strategies",
    x = "District",
    y = "Count",
    fill = "Sample Source"
  ) +
  theme_minimal() +
  theme(
    axis.text.y = element_text(size = 7),
    plot.title = element_text(face = "bold", size = 14),
    legend.position = "top"
  )
```

```{r}
library(dplyr)
library(ggplot2)

# Combine all four datasets with source labels
df_lienz_all <- bind_rows(
  eusilcA_pop %>% mutate(source = "Population"),
  eusilcA_smp %>% mutate(source = "Sample 1"),
  eusilcA_smp2 %>% mutate(source = "Sample 2"),
  eusilcA_smp3 %>% mutate(source = "Sample 3")
) %>%
  filter(district == "Lienz")

# Compute variance for each source
var_summary_all <- df_lienz_all %>%
  group_by(source) %>%
  summarise(variance = var(eqIncome, na.rm = TRUE))

# Plot
ggplot(df_lienz_all, aes(x = source, y = eqIncome, fill = source)) +
  geom_boxplot(alpha = 0.6, outlier.shape = NA) +
  geom_jitter(width = 0.2, alpha = 0.3, color = "black") +
  labs(
    title = "eqIncome Distribution in Lienz: Population vs Samples",
    x = "Data Source",
    y = "eqIncome"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    legend.position = "none"
  ) +
  geom_text(
    data = var_summary_all,
    aes(x = source, y = max(df_lienz_all$eqIncome, na.rm = TRUE) * 0.95,
        label = paste("Var =", round(variance, 0))),
    inherit.aes = FALSE,
    size = 4,
    fontface = "italic"
  )
```

```{r}
library(dplyr)
library(ggplot2)

# Combine all four datasets with source labels
df_wien_all <- bind_rows(
  eusilcA_pop %>% mutate(source = "Population"),
  eusilcA_smp %>% mutate(source = "Sample 1"),
  eusilcA_smp2 %>% mutate(source = "Sample 2"),
  eusilcA_smp3 %>% mutate(source = "Sample 3")
) %>%
  filter(district == "Wien")

# Compute variance for each source
var_summary_wien <- df_wien_all %>%
  group_by(source) %>%
  summarise(variance = var(eqIncome, na.rm = TRUE))

# Plot
ggplot(df_wien_all, aes(x = source, y = eqIncome, fill = source)) +
  geom_boxplot(alpha = 0.6, outlier.shape = NA) +
  labs(
    title = "eqIncome Distribution in Wien: Population vs Samples",
    x = "Data Source",
    y = "eqIncome"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    legend.position = "none"
  ) +
  geom_text(
    data = var_summary_wien,
    aes(x = source, y = max(df_wien_all$eqIncome, na.rm = TRUE) * 0.95,
        label = paste("Var =", round(variance, 0))),
    inherit.aes = FALSE,
    size = 4,
    fontface = "italic"
  )
```


```{r}
library(dplyr)
library(ggplot2)

# Combine datasets with source labels
df_lienz_all <- bind_rows(
  eusilcA_pop %>% mutate(source = "Population"),
  eusilcA_smp %>% mutate(source = "Sample 1"),
  eusilcA_smp2 %>% mutate(source = "Sample 2"),
  eusilcA_smp3 %>% mutate(source = "Sample 3")
) %>%
  filter(district == "Lienz")

# Compute summary stats
summary_stats <- df_lienz_all %>%
  group_by(source) %>%
  summarise(
    mean = mean(eqIncome, na.rm = TRUE),
    variance = var(eqIncome, na.rm = TRUE)
  )

# Density plot with mean lines and annotations
ggplot(df_lienz_all, aes(x = eqIncome, fill = source)) +
  geom_density(alpha = 0.4) +
  # Mean lines
  geom_vline(
    data = summary_stats,
    aes(xintercept = mean, color = source),
    linetype = "dashed",
    size = 1
  ) +
  # Mean labels
  geom_text(
    data = summary_stats,
    aes(x = mean, y = 0, label = paste("Mean =", round(mean, 0))),
    angle = 90,
    vjust = -0.5,
    hjust = 0,
    size = 3.5,
    fontface = "italic",
    color = "black",
    inherit.aes = FALSE
  ) +
  labs(
    title = "Density of eqIncome in Lienz: Population vs Samples",
    x = "eqIncome",
    y = "Density",
    fill = "Source",
    color = "Mean Line"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 14))
```

```{r}
library(dplyr)
library(ggplot2)

# Combine datasets with source labels
df_wien_all <- bind_rows(
  eusilcA_pop %>% mutate(source = "Population"),
  eusilcA_smp %>% mutate(source = "Sample 1"),
  eusilcA_smp2 %>% mutate(source = "Sample 2"),
  eusilcA_smp3 %>% mutate(source = "Sample 3")
) %>%
  filter(district == "Wien")

# Compute summary stats
summary_stats <- df_wien_all %>%
  group_by(source) %>%
  summarise(
    mean = mean(eqIncome, na.rm = TRUE),
    variance = var(eqIncome, na.rm = TRUE)
  )

# Density plot with mean lines and annotations
ggplot(df_wien_all, aes(x = eqIncome, fill = source)) +
  geom_density(alpha = 0.4) +
  
  # Mean lines
  geom_vline(
    data = summary_stats,
    aes(xintercept = mean, color = source),
    linetype = "dashed",
    size = 1
  ) +
  
  # Mean labels
  geom_text(
    data = summary_stats,
    aes(x = mean, y = 0, label = paste("Mean =", round(mean, 0))),
    angle = 90,
    vjust = -0.5,
    hjust = 0,
    size = 3.5,
    fontface = "italic",
    color = "black",
    inherit.aes = FALSE
  ) +
  
  labs(
    title = "Density of eqIncome in Wien: Population vs Samples",
    x = "eqIncome",
    y = "Density",
    fill = "Source",
    color = "Mean Line"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 14))
```


```{r}
library(dplyr)
library(ggplot2)

# Compute variance by district for each sample
var_df <- bind_rows(
  eusilcA_smp %>% mutate(source = "Sample 1"),
  eusilcA_smp2 %>% mutate(source = "Sample 2"),
  eusilcA_smp3 %>% mutate(source = "Sample 3")
) %>%
  group_by(source, district) %>%
  summarise(variance = var(eqIncome, na.rm = TRUE), .groups = "drop")

# Create ordering based on Sample 1
district_order <- var_df %>%
  filter(source == "Sample 1") %>%
  arrange(variance) %>%
  pull(district)

# Apply ordering
var_df <- var_df %>%
  mutate(district = factor(district, levels = district_order))

# Plot
ggplot(var_df, aes(x = district, y = variance, fill = source)) +
  geom_bar(stat = "identity", position = "dodge") +
  coord_flip() +
  labs(
    title = "Variance of eqIncome by District",
    subtitle = "Ordered by Sample 1 (smallest to largest)",
    x = "District",
    y = "Variance",
    fill = "Sample"
  ) +
  theme_minimal() +
  theme(
    axis.text.y = element_text(size = 7),
    plot.title = element_text(face = "bold", size = 14),
    legend.position = "top"
  )
```



```{r}

# ggplot(n_sample, aes(x = n)) +
#   geom_histogram(aes(fill = ..count..), binwidth = 5, color = "white") +
#   scale_fill_viridis_c(option = "D", direction = -1) +
#   labs(
#     title = "Distribution of Sample Sizes Across Districts",
#     x = "Number of Observations (n)",
#     y = "Count of Districts",
#     fill = "District Count"
#   ) +
#   theme_minimal()
```

We see that the observations are unequally distributed across the different districts. Furthermore, we have 24 districts that are not represented in the sample:

```{r}
# Check if all districts in the population are in the sample
table(eusilcA_popAgg$Domain %in% unique(eusilcA_smp$district))

t <- as.data.frame(unique(eusilcA_smp$district))
```

This is a relevant fact, since it will significantly affect the results and even the implementation of some SAE methods --remember that direct methods only generate outputs for areas with sampled observations.

Our target variable `eqIncome` follows a skewed distribution, with majority of individuals concentrated around lower income values (€ 20,000). We see very high agreement between sample values and population values.

```{r}
# ggplot() +
#   geom_density(data = eusilcA_smp, aes(x = eqIncome, color = "Sample", linetype = "Sample"), lwd = 1) +
#   geom_density(data = eusilcA_pop, aes(x = eqIncome, color = "Population", linetype = "Population"), lwd = 1) +
#   scale_linetype_manual(name = "", values = c("Population" = "dashed", "Sample" = "solid")) +
#   scale_color_manual(name = "", values = c("Population" = "#ca6702", "Sample" = "#69b3a2")) +
#   labs(
#     title = "Equivalised income distribution",
#     x = "eqIncome",
#     y = "density"
#   ) 
```

```{r}
# ggplot() +
#   geom_histogram(data = eusilcA_smp, aes(x = eqIncome, fill = "Sample"), 
#                  position = "identity", alpha = 0.5, bins = 50, color = "white") +
#   geom_histogram(data = eusilcA_pop, aes(x = eqIncome, fill = "Population"), 
#                  position = "identity", alpha = 0.5, bins = 50, color = "white") +
#   scale_fill_manual(name = "", values = c("Population" = "#ca6702", "Sample" = "#69b3a2")) +
#   labs(
#     title = "Equivalised Income Distribution",
#     x = "eqIncome",
#     y = "Count"
#   ) 
```

```{r}
library(ggplot2)
library(cowplot)

# Density plot
p_density <- ggplot() +
  geom_density(data = eusilcA_pop, aes(x = eqIncome, color = "Population", linetype = "Population"), lwd = 1) +
  geom_density(data = eusilcA_smp, aes(x = eqIncome, color = "Sample 1", linetype = "Sample 1"), lwd = 1) +
  geom_density(data = eusilcA_smp2, aes(x = eqIncome, color = "Sample 2", linetype = "Sample 2"), lwd = 1) +
  geom_density(data = eusilcA_smp3, aes(x = eqIncome, color = "Sample 3", linetype = "Sample 3"), lwd = 1) +
  scale_linetype_manual(name = "", values = c(
    "Population" = "dashed",
    "Sample 1" = "solid",
    "Sample 2" = "dotdash",
    "Sample 3" = "twodash"
  )) +
  scale_color_manual(name = "", values = c(
    "Population" = "#ca6702",
    "Sample 1" = "#69b3a2",
    "Sample 2" = "#577590",
    "Sample 3" = "#8e8e8e"
  )) +
  labs(
    title = "Equivalised Income Distribution (Density)",
    x = "eqIncome",
    y = "Density"
  ) +
  theme_minimal()

# Histogram plot
p_hist <- ggplot() +
  geom_histogram(data = eusilcA_pop, aes(x = eqIncome, fill = "Population"),
                 position = "identity", alpha = 0.4, bins = 50, color = "white") +
  geom_histogram(data = eusilcA_smp, aes(x = eqIncome, fill = "Sample 1"),
                 position = "identity", alpha = 0.4, bins = 50, color = "white") +
  geom_histogram(data = eusilcA_smp2, aes(x = eqIncome, fill = "Sample 2"),
                 position = "identity", alpha = 0.4, bins = 50, color = "white") +
  geom_histogram(data = eusilcA_smp3, aes(x = eqIncome, fill = "Sample 3"),
                 position = "identity", alpha = 0.4, bins = 50, color = "white") +
  scale_fill_manual(name = "", values = c(
    "Population" = "#ca6702",
    "Sample 1" = "#69b3a2",
    "Sample 2" = "#577590",
    "Sample 3" = "#8e8e8e"
  )) +
  labs(
    title = "Equivalised Income Distribution (Histogram)",
    x = "eqIncome",
    y = "Count"
  ) +
  theme_minimal()

# Combine plots
plot_grid(p_hist, p_density, labels = c("A", "B"), ncol = 1, align = "v")
```

```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
library(purrr)

# Add source labels
df1 <- eusilcA_smp %>% mutate(source = "Sample 1")
df2 <- eusilcA_smp2 %>% mutate(source = "Sample 2")
df3 <- eusilcA_smp3 %>% mutate(source = "Sample 3")

# Combine all samples
combined_df <- bind_rows(df1, df2, df3)

# Select numeric columns except 'weight', 'pop_n', and 'sample_n'
numeric_vars <- combined_df %>%
  select(where(is.numeric)) %>%
  select(-weight, -pop_n, -sample_n) %>%
  names()

# Split variables into 3 roughly equal groups
var_groups <- split(numeric_vars, ceiling(seq_along(numeric_vars) / (length(numeric_vars)/3)))

# Function to create violin plot for a group of variables
plot_violin_group <- function(vars, group_id) {
  standardized_df <- combined_df %>%
    select(all_of(vars), source) %>%
    pivot_longer(cols = -source, names_to = "variable", values_to = "value") %>%
    group_by(variable) %>%
    mutate(value_z = scale(value)) %>%
    ungroup()
  
  ggplot(standardized_df, aes(x = variable, y = value_z, fill = source)) +
    geom_violin(position = position_dodge(width = 0.8), trim = FALSE, scale = "width", alpha = 0.7) +
    scale_fill_manual(values = c(
      "Sample 1" = "#69b3a2",
      "Sample 2" = "#577590",
      "Sample 3" = "#8e8e8e"
    )) +
    labs(
      title = paste("Standardized Distributions — Variable Group", group_id),
      x = "Variable",
      y = "Standardized Value (Z-score)",
      fill = "Sample"
    ) +
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1),
      plot.title = element_text(face = "bold", size = 14),
      legend.position = "top"
    )
}

# Generate plots for each group
plots <- imap(var_groups, plot_violin_group)

# Display plots
plots[[1]]
plots[[2]]
plots[[3]]

```


Now that we have a better understanding of the data, we can start calculating our estimators.

# Direct estimator

We will start by computing the most simple SAE estimator -- the direct estimator. Direct estimators use only information collected from the domain of interest. They are relatively simple to obtain, since they use the sample weights and population values. However, they are very sensitive to small sample sizes.

We will calculate the direct estimator using the `sae` package. The `direct()` function computes the Horvitz-Thompson estimator of domain means using this formula:

$$
\hat{\bar{Y_d}} = \frac{1}{N_d} \sum_{i \in s_d} w_{di}Y_{di}
$$

where $N_d$ is the population at the domain of interest $d$; $s_d$ is the set of sampled observations in domain $d$; $w_{di}$ is the sample weight for unit $i$ in domain $d$; and $Y_{di}$ is the observation of the target variable for unit $i$ in domain $d$, for all $i$ in $S_d$. In addition to the direct estimator of the mean, the `direct()` function also gives us the standard deviation and coefficient of variation for each domain.

From our survey, we know that the `weights` column represent the survey weights, and that sampling was done without replacement. This information can usually be found in the documentation of the survey, together with any clusters or strata that might have been defined by the surveyors. Additionally, we need the total population sizes for each domain in the sample.


```{r}
# Load required package
library(ggplot2)

# Step 1: Define population
population <- c(2, 4, 6, 8, 10)

# Step 2: Set seed for reproducibility
set.seed(123)

# Step 3: Generate sample means

# With replacement
means_with <- replicate(1000, {
  s <- sample(population, size = 3, replace = TRUE)
  mean(s)
})

# Without replacement
means_without <- replicate(1000, {
  s <- sample(population, size = 3, replace = FALSE)
  mean(s)
})

# Step 4: Check for non-finite values
sum(!is.finite(means_with))     # Should be 0
sum(!is.finite(means_without))  # Should be 0

# Step 5: Combine into data frame
df <- data.frame(
  mean = c(means_with, means_without),
  method = rep(c("With Replacement", "Without Replacement"),
               times = c(length(means_with), length(means_without)))
)

# Step 6: Plot distributions
ggplot(df, aes(x = mean, fill = method)) +
  geom_density(alpha = 0.6, adjust = 1.5) +
  labs(title = "Distribution of Sample Means",
       x = "Sample Mean",
       y = "Density") +
  theme_minimal()
```

-   Scenario A: We have access for both sampling weights and domain sizes, and from the survey design we know that sampling was performed without replacement. In this case, the direct estimator is calculated as follows:

$$
\hat{\bar{Y_d}}^{dir} = \frac{1}{N_d} \sum_{i \in s_d} w_{di}y_{di}
$$ - $\hat{\bar{Y_d}}^{dir}$ is the direct estimator of the variable of interest. - $s_d$ is the sample in domain $d$. - $y_i$ is the value of the target variable for observation $i$ in domain $d$. - $w_{di}$ is the sampling weight of observation $i$. - $N_d$ is the population at domain $d$.

And the variance is computed following this equation:

$$
\text{Var}(\hat{Y}_d^{dir}) = \frac{\sum_{i \in s_d} w_i (w_i -1)y_i^2}{N_d^2}
$$

-   Scenario B: We have access to domain sizes, but not to the sampling weights, and we know the sampling was performed without replacement. In this case, we will ignore the sampling weights, just the sample size.

$$
\hat{Y}_d^{dir} = \frac{1}{n_d} \sum{i \in s_d} y_i 
$$

And the variance is:

$$
\text{Var}(\hat{Y}d^{dir}) = \frac{1 - f_d}{n_d} S{d}^2
$$

Where $f_d = \frac{n_d}{N_d}$ and $S_d^2$ is the sample variance of the domain. Since we are introducing sampling without replacement, the variance of our estimator is reduced, because each time we draw a new sample removes one possibility from the population, slightly reducing the uncertainty for each subsequent draw. The $f_d$ factor ensures the variance is appropriately reduced for large samples relative to the domain size.

-   Scenario C: We have access to sample weights, domain sizes, and know the sampling was done with replacement. In this case, the direct estimator is the same as in Scenario 1, since it also accounts for weights and uses domain sizes:

$$
\hat{\bar{Y_d}}^{dir} = \frac{1}{N_d} \sum_{i \in s_d} w_{di}y_{di}
$$

The variance, however, changes, since we do the sampling with replacement:

$$
\text{Var}(\hat{Y}d^{dir}) = \frac{1}{n_d} \sum_{i \in s_d} (f_d w_i y_i - \hat{Y}_d^{dir})^2
$$

-   Scenario D: we know that sampling was performed with replacement, but have no access to sample weights nor domain sizes. We calculate the estimator in the same way we did with Scenario B.

$$
\hat{Y}_d^{dir} = \frac{1}{n_d} \sum{i \in s_d} y_i 
$$

This time, the variance is calculated as follows:

$$
\text{Var}(\hat{Y}_d^{dir}) = \frac{S{d}^2}{n_d}
$$

The appropriate scenario depends on the availability of the data. Let's calculate the direct estimator assuming we have access to sample weights, domain sizes, and know the sampling was run without replacement.

```{r}

p <- as.data.frame(pop_area_size)

##AD - OK, tracked this down to a vector that comes from the package saeTrafo - this needs explanation. 
str(eusilcA_pop)

district_summary <- eusilcA_pop %>%
  group_by(district) %>%
  summarise(
    n = n(),
    across(where(is.numeric), mean, na.rm = TRUE)
  ) %>%
  arrange(desc(n))  # Optional: sort by count

##OK so pop_area_size is a count of the number of cases included in each district in the full eusilcA_pop file. 

# Clean and match sample population sizes
domsize_smp <- data.frame(
  district = names(pop_area_size),
  N = pop_area_size
) %>%
  filter(district %in% eusilcA_smp$district)

domsize_smp3 <- data.frame(
  district = names(pop_area_size),
  N = pop_area_size
) %>%
  filter(district %in% eusilcA_smp3$district)

eusilcA_smp$district <- droplevels(eusilcA_smp$district)

##domzise_smp only has 70 cases as it uses eusilcA_smp
```

```{r}
# Calculate direct estimator
dir_est_a1 <- sae::direct(
  y = eusilcA_smp$eqIncome, #vector specifying the individual values of the variable for which we want to estimate the domain means.
  dom = eusilcA_smp$district, # vector or factor (same size as y) with domain codes.
  sweight = eusilcA_smp$weight, # optional vector (same size as y) with sampling weights. When this argument is not included, by default estimators are obtained under simple random sampling (SRS).
  domsize = domsize_smp, # D*2 data frame with domain codes in the first column and the corresponding domain population sizes in the second column. This argument is not required when sweight is not included and replace=TRUE (SRS with replacement).
  replace = FALSE
)

# Format the direct estimator for future use
dir_est1 <- dir_est_a %>% 
  mutate(Var = SD^2) %>% 
  dplyr::select("Domain","Direct","Var", "SampSize")

head(dir_est)

library(ggplot2)

ggplot(dir_est1, aes(x = log(SampSize), y = log(Var))) +
  geom_point(color = "#2a9d8f", size = 3, alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE, color = "#264653", linetype = "dashed") +

  labs(
    title = "Variance vs Sample Size by Domain",
    x = "Sample Size",
    y = "Variance",
    caption = "Dashed line: LOESS smooth"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    axis.title = element_text(size = 12)
  )
```

```{r}
# Calculate direct estimator
dir_est_a3 <- sae::direct(
  y = eusilcA_smp3$eqIncome, #vector specifying the individual values of the variable for which we want to estimate the domain means.
  dom = eusilcA_smp3$district, # vector or factor (same size as y) with domain codes.
  sweight = eusilcA_smp3$weight, # optional vector (same size as y) with sampling weights. When this argument is not included, by default estimators are obtained under simple random sampling (SRS).
  domsize = domsize_smp3, # D*2 data frame with domain codes in the first column and the corresponding domain population sizes in the second column. This argument is not required when sweight is not included and replace=TRUE (SRS with replacement).
  replace = FALSE
)

# Format the direct estimator for future use
dir_est3 <- dir_est_a3 %>% 
  mutate(Var = SD^2) %>% 
  dplyr::select("Domain","Direct","Var", "SampSize")


head(dir_est3)

ggplot(dir_est3, aes(x = log(SampSize), y = log(Var))) +
  geom_point(color = "#2a9d8f", size = 3, alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE, color = "#264653", linetype = "dashed") +
  labs(
    title = "Variance vs Sample Size by Domain",
    x = "Sample Size",
    y = "Variance",
    caption = "Dashed line: LOESS smooth"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    axis.title = element_text(size = 12)
  )
```


### Task – How would you estimate the alternative direct estimators?

Hint: You can use the same formula we used for Scenario A, but changing the parameters that refer to the different alternatives. You can always have a look at what each parameter in the function does, and see some examples by running `?sae::direct()`.

```{r}
# Scenario B -- No replacement, with domain sizes, no weights

# Calculate direct estimator
dir_est_b <- sae::direct(
  y = eusilcA_smp$eqIncome, #vector specifying the individual values of the variable for which we want to estimate the domain means.
  dom = eusilcA_smp$district, # vector or factor (same size as y) with domain codes.
  domsize = domsize_smp, # D*2 data frame with domain codes in the first column and the corresponding domain population sizes in the second column. This argument is not required when sweight is not included and replace=TRUE (SRS with replacement).
  replace = FALSE
)

# Scenario C -- With replacement, weights and domain sizes

# Calculate direct estimator
dir_est_c <- sae::direct(
  y = eusilcA_smp$eqIncome, #vector specifying the individual values of the variable for which we want to estimate the domain means.
  dom = eusilcA_smp$district, # vector or factor (same size as y) with domain codes.
  sweight = eusilcA_smp$weight, # optional vector (same size as y) with sampling weights. When this argument is not included, by default estimators are obtained under simple random sampling (SRS).
  domsize = domsize_smp, # D*2 data frame with domain codes in the first column and the corresponding domain population sizes in the second column. This argument is not required when sweight is not included and replace=TRUE (SRS with replacement).
  replace = TRUE
)

# Scenario D -- With replacement, no weights nor domain sizes

# Calculate direct estimator
dir_est_d <- sae::direct(
  y = eusilcA_smp$eqIncome, #vector specifying the individual values of the variable for which we want to estimate the domain means.
  dom = eusilcA_smp$district,
  replace = TRUE
)

```

Once we have calculated the direct estimates, we can plot them in a map. We see that there are gaps in our data. This relates to direct estimators only being able to produce estimates based on domain-data only. For domains where sample data has no observations, the direct estimator will not produce an estimate. Additionally, the estimates produced for those areas for which sample sizes are small, will not be accurate.

```{r}
# Create data frame
dir_est_a_gdf <- merge(
  dir_est_a, 
  shape_austria_dis[,c("PB")], 
  by.x = "Domain",
  by.y = "PB") %>% 
  st_as_sf()

# Map
ggplot() +
  geom_sf(data = shape_austria_dis) +
  geom_sf(data = dir_est_a_gdf, aes(fill = Direct)) +
  labs(
    title = "Horvitz-Thompson Estimates"
  ) +
  theme_ipsum()
```

## Conclusion

We have seen how the direct estimate can produce estimates with minimal information. However, these results can be unreliable when sample sizes are small for our domain of interest, and they cannot be calculated if the domain of interest is not included in the sample. To solve this problem, we rely on model-based small area estimation methods.

# Model-based estimators

When samples are too small, or we have no observations for some domains, direct estimators will not provide us with accurate results, and we need to use model-based methods. There are two types of model-based methods – area- and unit-models. Area models incorporate domain-level information to produce more accurate estimates by "borrowing strength" from other areas, and using that information to improve the estimates for our small sample or out of sample areas. Similarly, unit-level models additionally incorporate unit-level auxiliary information to strengthen the estimates.

## Small Area Estimation in `R`

Before we start calculating our estimates, we will learn a bit more about the available `R` packages for small area estimation. In this practical we will be using two of the most commonly used packages for small area estimation modelling: `sae` and `SUMMER`. Although both packages aim at producing small area estimates in the presence of small sample sizes, they are based on two fundamentally different paradigms.

The `sae` package is based on frequentist methods, while `SUMMER` adopts a Bayesian approach to producing estimates. While in the frequentist approach the model estimates are assumed to be fixed, Bayesian models treat parameters as random variables. The Bayesian framework allows for incorporating prior information that allows for more flexible modelling, particularly in the presence of scarce or highly variable data, as is the case of small area estimation.

## Area-level model

Area-level estimates use domain-level data to improve the performance of direct estimates. The basic area-level model, the Fay-Herriot model, uses a two-step approach to do this: first, the model estimates a direct estimator in what is known as the *sampling model*. Next, the model computes area-specific random effects in what is known as the *linking model*.

The sampling model is formalised as follows:

$$
\hat{\theta}_i^{\mathrm{DIR}} = \theta_i + \epsilon_i; \quad \epsilon_i \sim^{\mathrm{ind}} \mathcal{N}(0, V_i), \quad i = 1, \ldots, M
$$

Where $V_i$ is the sampling variance of the direct estimator $\hat{\theta}_i^{\mathrm{DIR}}$ and $\epsilon_i$ represents the sampling error, which is assumed to be independently distributed.

The linking model allows us to borrow strength from other areas by assuming the small area parameter $\theta_i$ is related to auxiliary variables $x_i = (x_{i1}, x_{i2}, ..., x_{ip})'$ through a linear regression model given by:

$$
\theta_i = x_i' \beta + u_i = \alpha + \beta \bar{x_i}; \quad u_i \sim^{\mathrm{ind}} \mathcal{N}(0, \sigma_u^2), \quad i = 1, \ldots, M 
$$

where $\beta = (\beta_1, \beta_2..., \beta_p)'$ is a vector with the regression coefficients, and $u_i$ are area-specific random effects, also assumed to be independent and identically distributed (IID).

Although the basic Fay-Herriot model assumes the area-specific effects are independent, and identically distributed, we observe that this assumption, many times, does not hold. Often, we see that the values of our target variable in one domain are significantly correlated to those of other areas nearby. The error term in our model then becomes:

$$
u = \rho_1 Wu + \epsilon; \quad \epsilon \sim \mathcal{N}(0_i, \sigma_I^2 I_i)
$$ where $I_i$ is the identity matrix for the domains, and $0_i$ is a vector of zeros of the size of the total domains. Additionally, $\rho_1 \in (-1,1)$ is an autoregression parameter and $W$ is and adjacency matrix.

### Intercept-only model

The most basic area-level model is the intercept-only model. This model estimates a common mean for all estimates, but allows each of them to deviate from it based on an area-level random effect. The Fay-Herriot model "shrinks" those unreliable direct estimates –those with high variance– towards the global mean, producing more reliable estimates that borrow strength from other areas.

We model the direct survey estimate $\hat{Y}_i$ for each area $i$ as:

$$
\hat{Y}_i = \beta_0 + u_i + e_i
$$

Where:

\- $\hat{Y}_i$ is the direct survey estimate for area $i$

\- $\beta_0$ is the overall intercept (global mean)

\- $u_i \sim N(0, \sigma_u^2)$ area-specific random effect

\- $e_i \sim N(0, D_i)$ is the sampling error, with known variance $D_i$ from the survey.

This linear mixed model includes the fixed term $\beta_0$ which estimates the common mean, and the random part $u_i$ and $e_i$ that accounts for the area specific variation, and the variation due to the sampling error.

In `SUMMER`, the area-level model is calculated using the `smoothArea()` function.

```{r}
# Total population
domsize <- eusilcA_pop %>%
  mutate(district = as.character(district)) %>%
  group_by(district) %>%
  count(name = "size") %>%
  arrange(district)

# Scale the variable
eusilcA_smp2 <- eusilcA_smp
sd1 <- sd(eusilcA_smp2$eqIncome)
eusilcA_smp2$eqIncome <- eusilcA_smp2$eqIncome / sd1

# Produce direct estimate with re-scaled data
dir_est <- sae::direct(
  y = eusilcA_smp2$eqIncome,
  dom = eusilcA_smp2$district,
  sweight = eusilcA_smp2$weight,
  domsize = domsize_smp,
  replace = FALSE
) %>% 
  mutate(Var = SD^2) %>% 
  dplyr::select(Domain, Direct, Var)

# Fit area-level intercept-only model
fit <- smoothArea(
  formula = eqIncome ~ 1, # Intercept only
  domain = ~district,     # Domain of interest 
  direct.est = dir_est,   # Pre-computed direct estimator
  X.domain = domsize      # Add a matrix with all (even out-of-sample) domains
)

# Format results (un-scale them)
area_level_intercept <- fit$iid.model.est %>%
  mutate(
    mean   = mean * sd1,
    median = median * sd1,
    lower  = lower * sd1,
    upper  = upper * sd1,
    var    = var * sd1^2,
    se     = sqrt(var)
  )

# See results
head(area_level_intercept)
```

### Fay-Herriot model with auxiliary information

We can estimate a slightly more complex model by adding area-level covariates. The Fay-Herriot model will combine the direct estimates with area-level auxiliary data, in this case, the variables `cash`, `unempl_ben` and `tax_adj`. The model is estimated as follows:

$$
\hat{Y}_i = \beta_0 + \beta_1 \cdot \text{cash}_i + \beta_2 \cdot \text{unempl\_ben} + \beta_3 \cdot \text{tax\_adj} + u_i + e_i
$$

```{r}
# Produce covariate matrix and scale the values
# Define covariates
covariates <- c("cash", "unempl_ben", "tax_adj")

# Extract and scale covariates from eusilcA_popAgg
Xmat_scaled <- eusilcA_popAgg %>%
  dplyr::select(district = Domain, all_of(covariates)) %>%
  mutate(across(all_of(covariates), ~ . / sd(.)))

# Area-level estimate with covariates
fit <- smoothArea(
  formula = eqIncome ~ cash + unempl_ben + tax_adj,
  domain = ~district,
  direct.est = dir_est,
  return.samples = T,
  X.domain = Xmat_scaled
)

# Un-scale my outputs
area_level_covariates <- fit$iid.model.est %>%
  mutate(
    mean   = mean * sd1,
    median = median * sd1,
    lower  = lower * sd1,
    upper  = upper * sd1,
    var    = var * sd1^2,
    se     = sqrt(var)
  )

# See outputs
head(area_level_covariates)
```

### Spatial model

The last iteration for the area-level model involves incorporating the spatial component. This extension of the Fay-Herriot model assumes spatial correlation between areas, assuming that the values of one area will be influenced by the values of the areas around it.

$$
\boldsymbol{u} = \rho W \boldsymbol{u} + \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(\boldsymbol{0}, \sigma^2 I)
$$

```{r}
# Rename the proximity matrix's names and rows
colnames(eusilcA_prox) <- rownames(eusilcA_prox) <- eusilcA_popAgg$Domain

# Area-level model with spatial effects estimate
fit <- smoothArea(
  formula = eqIncome ~ cash + unempl_ben + tax_adj,
  domain = ~district,
  direct.est = dir_est,
  return.samples = T,
  X.domain = Xmat_scaled,
  adj.mat = eusilcA_prox   # Add adjacency matrix
)

# Un-scale my outputs
area_level_spatial <- fit$bym2.model.est %>%
  mutate(
    mean   = mean * sd1,
    median = median * sd1,
    lower  = lower * sd1,
    upper  = upper * sd1,
    var    = var * sd1^2,
    se     = sqrt(var)  
  )

head(area_level_spatial)
```

### Model comparison

Now that all the area-level models have been estimated, we can compare the results. For all three models, we observe high correlation of the estimates

```{r, message=FALSE, fig.width=10, fig.height=8}
# Prepare data for model comparison
area_level_summer_est <- data.frame(
  Domain = area_level_intercept$domain,
  Intercept_model_est = area_level_intercept$mean,
  Covariates_model_est = area_level_covariates$mean,
  Spatial_model_est = area_level_spatial$mean)


area_level_summer_se <- data.frame(
  Domain = area_level_intercept$domain,
  Intercept_model_se = area_level_intercept$se,
  Covariates_model_se = area_level_covariates$se,
  Spatial_model_se = area_level_spatial$se
)

# Flag out-of-sample domains
oos <- setdiff(eusilcA_pop$district,eusilcA_smp$district)
area_level_summer_est <- area_level_summer_est %>%
  mutate(sample_status = if_else(Domain %in% oos, "out-of-sample", "in-sample"))

area_level_summer_se <- area_level_summer_se %>%
  mutate(sample_status = if_else(Domain %in% oos, "out-of-sample", "in-sample"))

# Plot
ggpairs(
  data = area_level_summer_est[,-1],
  aes(color = sample_status)
) +
  theme_ipsum()

ggpairs(
  data = area_level_summer_se[,-1],
  aes(color = sample_status)
) +
  theme_ipsum()

```

We can also map the outputs of the different models. We observe high agreement between the auxiliary data and the spatial model, with very similar estimated values. These results differ from the intercept-only model, where we observe less variability in the outputs.

```{r, message=FALSE, fig.width=8, fig.height=12}
# Add geometry
area_level_summer_gdf <- merge(
  area_level_summer_est,
  shape_austria_dis[,c("PB")],
  by.x = "Domain",
  by.y = "PB"
) %>% 
  st_as_sf()

# Make data long
area_level_long <- area_level_summer_gdf %>%
  pivot_longer(
    cols = c(Intercept_model_est, Covariates_model_est, Spatial_model_est),
    names_to = "Model",
    values_to = "Estimate"
  )

# Order the models in increasing complexity
area_level_long$Model <- factor(area_level_long$Model,
                                levels = c("Intercept_model_est",
                                           "Covariates_model_est",
                                           "Spatial_model_est"))

# Map
ggplot() +
  geom_sf(data = area_level_long, aes(fill = Estimate), col = NA) +
  facet_wrap(~Model, ncol = 1, nrow = 3) +
  theme_ipsum()
```

```{r, fig.width=8, fig.height=12}
area_level_summer_sd <- data.frame(
  Domain = area_level_intercept$domain,
  Intercept_model_sd = sqrt(area_level_intercept$var),
  Covariates_model_sd = sqrt(area_level_covariates$var),
  Spatial_model_sd = sqrt(area_level_spatial$var)
)

# Add geometry
area_level_summer_gdf <- merge(
  area_level_summer_sd,
  shape_austria_dis[,c("PB")],
  by.x = "Domain",
  by.y = "PB"
) %>% 
  st_as_sf()

# Make data long
area_level_long_sd <- area_level_summer_gdf %>%
  pivot_longer(
    cols = c(Intercept_model_sd, Covariates_model_sd, Spatial_model_sd),
    names_to = "Model",
    values_to = "SD"
  )

# Order the models by increasing complexity
area_level_long_sd$Model <- factor(area_level_long_sd$Model,
                                   levels = c("Intercept_model_sd",
                                              "Covariates_model_sd",
                                              "Spatial_model_sd"))

# Map SDs
ggplot() +
  geom_sf(data = area_level_long_sd, aes(fill = SD), col = NA) +
  facet_wrap(~Model, ncol = 1, nrow = 3) +
  labs(title = "Standard Deviation of Estimates by Model") +
  theme_ipsum()
```

### Compare models with true data

Something we can do in this practical is compare how our estimates compare to the true population ones provided in the `eusilcA_pop` data frame. Although we cannot perform this exercise in real life, since we will not have access to the true population values, we can run it here to illustrate how different models improve our estimates.

The correlation plots show how agreement between true values and estimates increases as model complexity also increases. This is in line with what we saw in the theoretical part of this workshop --more sophisticated models will result in more precise estimates.

```{r, fig.width=8, fig.height=12}
# True values from population
true_values <- eusilcA_pop %>%
  group_by(district) %>%
  summarise(
    true_income = mean(eqIncome, na.rm = TRUE)
  )


# Prepare model estimates
cov <- area_level_covariates %>%
  rename(district = domain, cov_mean = mean) %>%
  select(district, cov_mean)

int <- area_level_intercept %>%
  rename(district = domain, int_mean = mean) %>%
  select(district, int_mean)

spa <- area_level_spatial %>%
  rename(district = domain, spa_mean = mean) %>%
  select(district, spa_mean)

# Merge everything into one data frame
merged_df <- list(true_values, cov, int, spa) %>%
  reduce(full_join, by = "district")

# Create comparison plots
int_vs_true <- ggplot(merged_df) +
  geom_point(aes(x = true_income, y = int_mean)) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  scale_color_viridis_c() +
  labs(x = "True eqIncome", y = "Intercept-only estimate") +
  theme_minimal()

cov_vs_true <- ggplot(merged_df) +
  geom_point(aes(x = true_income, y = cov_mean)) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
    scale_color_viridis_c() +
  labs(x = "True eqIncome", y = "Covariates model estimate") +
  theme_minimal()

spa_vs_true <- ggplot(merged_df) +
  geom_point(aes(x = true_income, y = spa_mean)) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  scale_color_viridis_c() +
  labs(x = "True eqIncome", y = "Spatial model estimate") +
  theme_minimal()

# Combine plots and visualise
((int_vs_true) /
(cov_vs_true) /
(spa_vs_true)
)
```

## Unit-level model

The last model we will explore in this practical is the unit-level model. This model incorporates both area and unit-level covariates to improve the performance of our estimator.

The unit-level model works similarly to the area-level model:

-   First, it calculates a direct estimator of the target variable.

-   Next, it fits a model where the direct estimator acts as the dependent variable.

This time, the model incorporates both area- and unit-level errors:

$$
y_{id} = \mathbf{x}_{id}^\top \boldsymbol{\beta} + u_d + \epsilon_{id}
$$

where $u_d$ is the area-level error, and $\epsilon_{id}$ is the unit-level error.

Like in the area-level model, the unit-level model can make different assumptions about the area-level error distribution. If we assume they are independent and identically distributed, it will compute an IID, whereas if we suspect there might be spatial autocorrelation, we will calculate the BYM2 model.

For this example, we will assume that the sample data contains no out-of-sample domains. We need to make this assumption because for out-of-sample domains, we do not have access to individual-level data if we want to use the `eusilc` data set.

The function we will use to calculate the unit-level estimates is `smoothUnit()`. This function requires an explicit survey design, an object that describes the way the data was collected in the sample. We obtain this object with the `svydesign()` function. This function allows for both the inclusion of covariates and an adjacency matrix in order to account for spatial effects. In this example, we will calculate the unit-level model with two unit-level covariates.

```{r}
# Survey design
eusilcA_smp2$id <- 1:nrow(eusilcA_smp2) # Create id column for sample data
design <- svydesign(
  ids = ~id,
  weights = ~weight,
  data = eusilcA_smp2
)

# Unit level covariates
# Select unit-level covariates
covariates <- c("cash", "unempl_ben", "tax_adj")

# Then proceed with scaling the covariates
Xunit_scaled <- eusilcA_smp2 %>%
  dplyr::select(district = district, all_of(covariates)) %>%
  mutate(across(all_of(covariates), ~ . / sd(.)))

# Adjacency matrix with only in-sample domains
# Get the unique districts from the sample data
valid_districts <- unique(eusilcA_smp$district)

# Subset the proximity matrix to only in-sample domains
eusilcA_prox_filtered <- eusilcA_prox[rownames(eusilcA_prox) %in% valid_districts, 
                                      colnames(eusilcA_prox) %in% valid_districts]


# Fit model
fit <- smoothUnit(
  formula = eqIncome ~ cash + unempl_ben + tax_adj,
  domain = ~district,
  design = design,
  adj.mat = eusilcA_prox_filtered,
  X.pop = Xunit_scaled,
  domain.size = domsize
)

# Estimates
unit_level <- fit$bym2.model.est %>%
  mutate(
    mean   = mean * sd1,
    median = median * sd1,
    lower  = lower * sd1,
    upper  = upper * sd1,
    var    = var * sd1^2,
    se     = sqrt(var)
  )

head(unit_level)
```

# Frequentist approach

Now that we have computed our Bayesian small area estimates, we can try using the frequentist approach. In this example, we will be using the functions available through the `sae` package. An important consideration when using this functions is that they do not directly predict the estimates for out-of-sample domains like `SUMMER`. We have to compute them manually using the estimates provided by our fitted models.

## Area-level Model

### Intercept-only model

We begin by computing the direct estimates, using `sae::direct()` like we did in the Bayesian approach. This time, we use the function `sae::mseFH()` to estimate the Fay-Herriot model, in this case, just adding the intercept and no area-level covariates. This model assumes all domain-level means are drawn from a common distribution, centered on the intercept. The fitted model will give us the EBLUPs (Empirical Best Linear Unbiased Predictors) for sampled domains, and the RMSE (root mean squared errors) as a measure of uncertainty.

Since the `sae` package does not automatically provide estimates for out-of-sample estimates, we generate synthetic estimates for those by using the model's intercept and its standard error.

```{r}
# Direct estimator with untransformed data
dir_est <- sae::direct(
  y = eusilcA_smp$eqIncome,
  dom = eusilcA_smp$district,
  sweight = eusilcA_smp$weight,
  domsize = domsize_smp,
  replace = FALSE
) %>% 
  mutate(Var = SD^2) %>% 
  dplyr::select(Domain, Direct, Var)

# Fit intercept-only Fay-Herriot model (no covariates)
fh <- sae::mseFH(
  formula = dir_est$Direct ~ 1,
  vardir = dir_est$Var
)

# Extract EBLUPs and RMSEs for sampled domains
fh_df <- data.frame(
  Domain = dir_est$Domain,
  Estimate = fh$est$eblup,
  Estimator_Type = "EBLUP",
  RMSE = sqrt(fh$mse)
)

# Extract model intercept and standard error
intercept <- fh$est$fit$estcoef["X", "beta"]
se <- fh$est$fit$estcoef["X", "std.error"]

# Identify out-of-sample domains
oos_domains <- setdiff(eusilcA_popAgg$Domain, dir_est$Domain)

# Create synthetic estimates for OOS domains
oos_estimates <- data.frame(
  Domain = oos_domains,
  Estimate = intercept,
  Estimator_Type = "Synthetic",
  RMSE = se
)

# Combine sampled and out-of-sample estimates
fh_area_level_intercept <- bind_rows(fh_df, oos_estimates) %>% 
  arrange(Domain)

head(fh_area_level_intercept)
```

### Fay-Herriot model with auxiliary variables

To make the model more sophisticated, we can add area-level covariates. In this example we add a single area-level covariate, `cash`. The process mirrors the intercept-only model, but now we let our estimates vary with the covariate variable. Similarly, for within-sample domains we compute the EBLUP and RMSE, and for out-of-sample domains we manually compute the synthetic estimates by plugging the out-of-sample covariate values into the model.

```{r}
# Add more covariates
X_covar <- merge(dir_est, eusilcA_popAgg[,c("Domain","cash")])

# Fit model with covariates
fh_cov <- mseFH(
  formula = Direct ~ cash,
  vardir = Var,
  data = X_covar
)

# Format it as a table
fh_cov_df <- data.frame(
  "Domain" = dir_est$Domain,
  EBLUP = fh_cov$est$eblup,
  RMSE = sqrt(fh_cov$mse)
)

# Extract parameter estimates
intercept <- fh_cov$est$fit$estcoef["X(Intercept)", "beta"]
slope_cash <- fh_cov$est$fit$estcoef["Xcash", "beta"]
se_intercept <- fh_cov$est$fit$estcoef["X(Intercept)", "std.error"]

# Prepare covariates for all domains
X_all <- eusilcA_popAgg[, c("Domain", "cash")]

# Predict synthetic estimate for all areas
X_all <- X_all %>%
  mutate(
    Estimate = intercept + slope_cash * cash
  )

# Mark sample status
X_all$in_sample <- X_all$Domain %in% dir_est$Domain

# Add model EBLUPs and RMSEs for in-sample areas
X_all <- left_join(X_all, fh_cov_df, by = "Domain", suffix = c("_synthetic", "_eblup"))

# Combine estimates
fh_area_level_covariates <- X_all %>%
  mutate(
    Final_Estimate = ifelse(!is.na(EBLUP), EBLUP, Estimate),
    Estimator_Type = ifelse(!is.na(EBLUP), "EBLUP", "Synthetic"),
    Final_RMSE = ifelse(!is.na(RMSE), RMSE, se_intercept)
  ) %>%
  dplyr::select(Domain, Final_Estimate, Estimator_Type, Final_RMSE) %>% 
  arrange(Domain)

# See output
head(fh_area_level_covariates)
```

### Compare models

To compare both models, we create a pairwise correlation plot. First, we compare the estimates produced by both models, and observe there is good agreement between them. For out-of-sample domains, we calculated this synthetically, using the model's intercept as a reference. The RMSE is a metric to evaluate the accuracy of a model that compares the model's predictions with the observed values. We see that precision increases for the model including covariates, which suggests that this model performs better than the intercept-only one, as it is expected.

```{r, fig.width=10, fig.height=8}
# Make sure both dfs are in the same order
identical(fh_area_level_intercept$Domain, fh_area_level_covariates$Domain)

# Create data frame for comparison
fh_comparison <- data.frame(
  Domain = fh_area_level_intercept$Domain,
  Intercept_model_est = fh_area_level_intercept$Estimate,
  Covariates_model_est = fh_area_level_covariates$Final_Estimate,
  Intercept_model_rmse = fh_area_level_intercept$RMSE,
  Covariates_model_rmse = fh_area_level_covariates$Final_RMSE
)

# Flag out-of-sample domains
oos <- setdiff(eusilcA_pop$district,eusilcA_smp$district)
fh_comparison <- fh_comparison %>%
  mutate(sample_status = if_else(Domain %in% oos, "out-of-sample", "in-sample"))

# Compare estimates
ggplot(data = fh_comparison) +
  geom_point(aes(x = Intercept_model_est, y = Covariates_model_est, col = sample_status)) +
  theme_minimal()

# Compare RMSEs
ggplot(data = fh_comparison) +
  geom_point(aes(x = Intercept_model_rmse, y = Covariates_model_rmse, col = sample_status)) +
  geom_abline(slope = 1, intercept = 0) +
  theme_minimal()
```
